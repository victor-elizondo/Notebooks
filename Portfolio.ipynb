{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. General use"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train and Test Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(['target'], axis=1) #complete here the input \n",
    "y = df['target']#complete here the target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)\n",
    "\n",
    "# Standarize variables\n",
    "# Notice to prevent data leakage from the test set, \n",
    "# we only fit our scaler to the training set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop('TARGET CLASS',axis=1)) # Fit scaler to the features\n",
    "scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1)) # transform the features to a scaled version\n",
    "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df.drop('TARGET CLASS',axis=1)) # Fit scaler to the features\n",
    "scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1)) # transform the features to a scaled version\n",
    "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\n",
    "\n",
    "\n",
    "### Code Categorical variables in \n",
    "# in one step\n",
    "cat_feats = ['cat_feat1','cat_feat2'] #list with categorical features\n",
    "final_data = pd.get_dummies( df, columns=cat_feats, drop_first=True)\n",
    "# getting first the dummies to explore\n",
    "dummies = pd.get_dummies(df[cat_feats],drop_first=True)\n",
    "df = pd.concat([df.drop('cat_feats',axis=1),dummies],axis=1)\n",
    "\n",
    "# get metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print('-----------------------------------------------------')\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "# Merge dataframes (Join)\n",
    "df = pd.merge(df,movie_titles,on='item_id')\n",
    "\n",
    "# Create new variables (feature engineering)\n",
    "# alternative 1\n",
    "df['equal_or_lower_than_4?'] = df['set_of_numbers'].apply(lambda x: 'True' if x <= 4 else 'False')\n",
    "# alternative 2\n",
    "df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})\n",
    "\n",
    "\n",
    "# Count the not na and calculate percentage\n",
    "((len(df)-df.count())/len(df)*100).sort_values(ascending=False)\n",
    "\n",
    "# calculate the mean of a variable, respect to other correlated by groups\n",
    "df.groupby('total_acc')['mort_acc'].mean()\n",
    "\n",
    "# Apply a function to column col using two or more columns \n",
    "df['col'] = df.apply(func, axis=1)\n",
    "\n",
    "# Pass variables in an Apply statement\n",
    "df[cols].apply(lambda x: apply_join(x,sep),axis=1)\n",
    "df[cols].apply(func,args=[sep],axis=1)\n",
    "    def func(x, sep) # Consider define the function in this way\n",
    "\n",
    "#Select columns by its type\n",
    "df.select_dtypes(include='object').info()\n",
    "\n",
    "# to replace specific values\n",
    "df['col']=df['col'].replace(['NONE', 'ANY'], 'OTHER')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Supervised Algorithms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "predict = lm.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predict))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predict))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predict)))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1) #n_neighbors define la cantidad de vecinos\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "# Choose the right n_neighbors\n",
    "error = []\n",
    "for i in range(1,50):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    error.append(np.mean(predictions != y_test))\n",
    "    \n",
    "# Plot elbow plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(1,50),error,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Trees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "predictions = dtree.predict(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "# GRID Search (to  optimize the C and Gamma SCV parameters)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True,verbose=3)\n",
    "grid.fit(X_train,y_train) # May take awhile!\n",
    "grid.best_params_  #see the best parameters found by the grid\n",
    "grid.best_estimator_ #see the best estimator found by the grid\n",
    "grid_predictions = grid.predict(X_test) # Predict again with best parameters\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Unsupervised Algorithms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K Means"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(df)\n",
    "kmeans.cluster_centers_  # get the centers\n",
    "kmeans.labels_  #get the labels\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Principal Component Analysis (for feature extraction)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# La data debe estar escalada\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "scaled_data = scaler.transform(df)\n",
    "\n",
    "# Luego PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # NÃºmero de componentes que se quieren dejar\n",
    "pca.fit(scaled_data) # se calculan los componentes\n",
    "x_pca = pca.transform(scaled_data) # Se aplica la transformacion\n",
    "x_pca.shape # Con esto se puede ver la nueva dimensionalidad\n",
    "\n",
    "\n",
    "# Una manera interesante de ver como cada feature inicial aporta a cada Principal Component\n",
    "# es mediante un Heatmap de las features orig. vs. los Principal Components\n",
    "pca.components_\n",
    "df_comp = pd.DataFrame(pca.components_,columns=df['feature_names'])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma',)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recommender Systems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simple Recommender by giving similar movies (content based)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Now let's create a matrix that has the user ids on one access and the movie \n",
    "# title on another axis. Each cell will then consist of the rating the user \n",
    "# gave to that movie. Note there will be a lot of NaN values, because most \n",
    "# people have not seen most of the movies.\n",
    "moviemat = df.pivot_table(index='user_id',columns='title',values='rating')\n",
    "ratings.sort_values('num of ratings',ascending=False).head(10)\n",
    "\n",
    "# Let's choose two movies: starwars, a sci-fi movie. And Liar Liar, a comedy.\n",
    "# Now let's grab the user ratings for those two movies:\n",
    "starwars_user_ratings = moviemat['Star Wars (1977)']\n",
    "liarliar_user_ratings = moviemat['Liar Liar (1997)']\n",
    "\n",
    "# We can then use corrwith() method to get correlations between two pandas series:\n",
    "similar_to_starwars = moviemat.corrwith(starwars_user_ratings)\n",
    "similar_to_liarliar = moviemat.corrwith(liarliar_user_ratings)\n",
    "\n",
    "#Let's clean this by removing NaN values and using a DataFrame instead of a series:\n",
    "corr_starwars = pd.DataFrame(similar_to_starwars,columns=['Correlation'])\n",
    "corr_starwars.dropna(inplace=True)\n",
    "\n",
    "# Now if we sort the dataframe by correlation, we should get the most similar \n",
    "# movies, however note that we get some results that don't really make sense. \n",
    "# This is because there are a lot of movies only watched once by users who also\n",
    "# watched star wars (it was the most popular movie).\n",
    "corr_starwars.sort_values('Correlation',ascending=False).head(10)\n",
    "\n",
    "#Let's fix this by filtering out movies that have less than 100 reviews \n",
    "# (this value was chosen based off the histogram from earlier).\n",
    "# Now sort the values and notice how the titles make a lot more sense:\n",
    "corr_starwars = corr_starwars.join(ratings['num of ratings'])\n",
    "corr_starwars[corr_starwars['num of ratings']>100].sort_values('Correlation',ascending=False).head()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Otro recomendador mas avanzado usando Memory-Based Collaborative Filtering\n",
    "# en el notebook 02-Advanced Recommender Systems with Python.ipynb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLP\n",
    "More interesting resources in 01-NLP (Natural Language Processing) with Python.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "# define a function to pre-process de text\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Might take awhile...\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])\n",
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)\n",
    "\n",
    "# After the counting, the term weighting and normalization can be done with TF-IDF, \n",
    "# using scikit-learn's TfidfTransformer.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "\n",
    "# To transform the entire bag-of-words corpus into TF-IDF corpus at once:\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "\n",
    "# alternative you can use directly fit_transform() method\n",
    "\n",
    "# We'll be using scikit-learn here, choosing the Naive Bayes classifier to start with:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])\n",
    "\n",
    "all_predictions = spam_detect_model.predict(messages_tfidf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(messages['label'], all_predictions))\n",
    "\n",
    "# To repeat the process is better to create a pipeline and then make the data go\n",
    "# throught the pipeline like any sklearn Estimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB())  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "pipeline.fit(msg_train,label_train)\n",
    "predictions = pipeline.predict(msg_test)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorFlow + Keras (features standarizes)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# Is important to Convert Pandas to Numpy for Keras\n",
    "# Features\n",
    "X = df[['feature1','feature2']].values\n",
    "# Label\n",
    "y = df['price'].values\n",
    "\n",
    "\n",
    "# Basic model, adding layer one-by-one \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "\n",
    "# Final output node for prediction\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='mse')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Choosing an optimizer and loss\n",
    "\n",
    "Keep in mind what kind of problem you are trying to solve:\n",
    "\n",
    "    # For a multi-class classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # For a binary classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # For a mean squared error regression problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='mse')\n",
    "                  \n",
    "#### Training\n",
    "\n",
    "Below are some common definitions that are necessary to know and understand to correctly utilize Keras:\n",
    "\n",
    "* Sample: one element of a dataset.\n",
    "    * Example: one image is a sample in a convolutional network\n",
    "    * Example: one audio file is a sample for a speech recognition model\n",
    "* Batch: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).\n",
    "* Epoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n",
    "* When using validation_data or validation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.\n",
    "* Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=25,\n",
    "          batch_size=256,\n",
    "          validation_data=(X_test, y_test), \n",
    "          )\n",
    "\n",
    "# Evaluate\n",
    "model.history.history['loss'] # To get the loss history\n",
    "\n",
    "# Calculate the loss for every set\n",
    "training_score = model.evaluate(X_train,y_train,verbose=0)\n",
    "test_score = model.evaluate(X_test,y_test,verbose=0)\n",
    "# Or better get the values from history\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "\n",
    "# Predict (it has to be standarized)\n",
    "test_predictions = model.predict(X_test)\n",
    "### use predict_classes when is classification problem\n",
    "#predictions = model.predict_classes(X_test)\n",
    "\n",
    "# To calculate the error\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "test_predictions = pd.Series(test_predictions.reshape(300,))\n",
    "pred_df = pd.concat([pred_df,test_predictions],axis=1)\n",
    "pred_df.columns = ['Test Y','Model Predictions']\n",
    "\n",
    "mean_absolute_error(pred_df['Test Y'],pred_df['Model Predictions'])\n",
    "mean_squared_error(pred_df['Test Y'],pred_df['Model Predictions'])\n",
    "\n",
    "# And plot the error\n",
    "pred_df['Error'] = pred_df['Test Y'] - pred_df['Model Predictions']\n",
    "sns.distplot(pred_df['Error'],bins=50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predicting new data is important use the same scaler to standarize\n",
    "# important to shape the data when is only one sample (column to vector)\n",
    "model.predict_classes(new_gem.values.reshape(1,78))\n",
    "\n",
    "new_gem = scaler.transform(new_gem)\n",
    "model.predict(new_gem)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Saving and Loading a Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "later_model = load_model('my_model.h5')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Early Stopping\n",
    "Stop training when a monitored quantity has stopped improving.\n",
    " \n",
    "* Arguments:\n",
    "        monitor: Quantity to be monitored.\n",
    "        min_delta: Minimum change in the monitored quantity\n",
    "            to qualify as an improvement, i.e. an absolute\n",
    "            change of less than min_delta, will count as no\n",
    "            improvement.\n",
    "        patience: Number of epochs with no improvement\n",
    "            after which training will be stopped.\n",
    "        verbose: verbosity mode.\n",
    "        mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
    "            training will stop when the quantity\n",
    "            monitored has stopped decreasing; in `max`\n",
    "            mode it will stop when the quantity\n",
    "            monitored has stopped increasing; in `auto`\n",
    "            mode, the direction is automatically inferred\n",
    "            from the name of the monitored quantity."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', \n",
    "                           verbose=1, patience=25)\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop]\n",
    "          )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Early Stopping and Dropout"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dropout(0.5)) # Dropping 50% of the units randomly\n",
    "\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5)) # Dropping 50% of the units randomly\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Some visualization aids"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scatter between 2 variables, with hue obtained from get_dummies ######\n",
    "gg = final_data.plot(kind='scatter', y='F.Undergrad', x='Outstate',c=final_data['Private_Yes'], \n",
    "                        colormap ='coolwarm',colorbar=False,figsize=(10,10), grid=True)\n",
    "gg.set(xlabel=\"Outstate\", ylabel=\"F.Undergrad\")\n",
    "\n",
    "# ---- other alternative\n",
    "sns.lmplot('Room.Board','Grad.Rate',data=df, hue='Private',\n",
    "           palette='coolwarm',size=6,aspect=1,fit_reg=False)\n",
    "########################################################################\n",
    "\n",
    "# Histogram of 'Outstate' with hue of 'Private' using FacetGrid and controling the alpha\n",
    "g = sns.FacetGrid(data,hue=\"Private\",palette='viridis',height=6,aspect=2)\n",
    "g = g.map(plt.hist,'Outstate',bins=20,alpha=0.5)\n",
    "g.add_legend()\n",
    "\n",
    "# Plot correlation of the target vs all other features\n",
    "df.corr()['target'].sort_values().plot(kind='bar')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda419b29b9c0944154bb9c3485567e2d77"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}